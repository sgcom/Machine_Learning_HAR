---
title: "Human Activity Recognition - Eercise in Machine Learning with R"
author: "Stefan Ganev"
date: "December, 2016"
output: html_document
---

### Introduction

This is an exercise in machine learning with R, based on the Human Activity Recognition dataset referred here:

 > Ugulino, W.; Cardador, D.; Vega, K.; Velloso, E.; Milidiu, R.; Fuks, H. Wearable Computing: Accelerometers' Data Classification of Body Postures and Movements. Proceedings of 21st Brazilian Symposium on Artificial    Intelligence. Advances in Artificial Intelligence - SBIA 2012. In: Lecture Notes in Computer Science. , pp. 52-61. Curitiba, PR: Springer Berlin / Heidelberg, 2012. ISBN 978-3-642-34458-9. DOI: 10.1007/978-3-642-34459-6_6. 

HTTP Links to the original study:

 - http://groupware.les.inf.puc-rio.br/har
 - http://groupware.les.inf.puc-rio.br/har#ixzz4bhloUCRz
 - http://groupware.les.inf.puc-rio.br/public/2012.SBIA.Ugulino.WearableComputing-Presentation.pdf

The purpose of my work here is:

 - Try to reproduce similar performance to the one reported by the authors while starting from scratch, using the original data set, and applying well known machine learning libraries and techniques in R.
 

### Background

The authors of the original study attach accelerometer sensors to 5 people, collect data, and apply supervised machine learning to recognize the type of activity the test subjects perform. The 5 activite classes are sitting-down, standing-up, standing, walking, and sitting. 

The authors used combination of module of acceleration vector and variance for the separate sensors and combined for all sensors - mean and standard deviation of (M1+M2+M3+M4).

Benchmark provided by the authors ( C4.5 decision tree with the AdaBoost ensemble method):

```{}
Detailed Accuracy

Correctly Classified Instances   164662 99.4144 %
Incorrectly Classified Instances    970  0.5856 %
Root mean squared error                  0.0463	
Relative absolute error                  0.7938 %	
Relative absolute error                  0.7938 %	

Detailed Accuracy by Class

TP Rate	FP Rate	Precision	Recall	F-Measure	ROC Area	Class
0.999     0        1       0.999     0.999       1    Sitting
0.971 0.002    0.969       0.971     0.970   0.999    Sitting down
0.999 0.001    0.998       0.999     0.999       1    Standing
0.962 0.003    0.969       0.962     0.965   0.999    Standing up
0.998 0.001    0.998       0.998     0.998       1    Walking
0.994 0.001    0.994       0.994     0.994       1    Weighted Avg.

Ref: http://groupware.les.inf.puc-rio.br/har#dataset#ixzz3qX9tzn6g
```

This is a supervised machine learning problem, more specifically - classification. 


What is interesting about this dataset:
 - it is real data;
 - the problem domain is interesting one: wearable sensors, IoT, physical computing;
 - could be applied for good cause, for exsample to improve quality of life.


The goal of this exercise is
 - train a predictive ML model that would be able to recognize the type of activity with good accuracy
 - start from scratch and raw data, use R and appropriate libraries
 - try to achieve performance measures (accuracy, etc) close to author's benchmark


### Reading the data set, checking dimensions and data types

```{r}
rm(list=ls())
csv.data.file <- "data/dataset-har-PUC-Rio-ugulino.csv"
full.data <- read.csv(file=csv.data.file, header=T, sep=";")
dim(full.data)
str(full.data)
head(full.data)
summary(full.data)
```

### Libraries to use

```{r message=FALSE}
library(caret)
library(corrplot )
```

Other libraries for the specific statistical models will be loaded automatically on demand.

### Common-senseFeature Selection

The data set includes person name, gender, age, height, weight, bmi, and the x/y/z readings from the 4 accellerometers.

The person's name is obviously to be removed from the attribute set since it is simply personal identifier with no relation to the model.

I choose to also consider all other person-specific variables irrelevant with the following resoning:

 - We are trying to buil a general model that coud be useful for the general population.

 - Some of the personal attributes (height, weight) could be potentially relevant, however we have another problem: only 4 people participated in this study. The range in height, weight, etc. is not representative for the overal population.

With all that in mind, the following will be removed for the purpose of modeling:

 - gender
 - age
 - height
 - weight
 - body mass index

So, the variables to be used are the following:

```{r}
vars.to.use <- setdiff( names(full.data), 
                        c("user","gender","age","how_tall_in_meters","weight","body_mass_index"))
outcome <- "class"
predictors <- setdiff(vars.to.use, outcome)
har.data <- full.data[, vars.to.use]
```

The derived dataset `har.data` now includes only the measurements from the four accelerometers, each having x, y, and z values plus the outcome variable.


### Checking out the data we have at this point

Looking what we have at very high level:

```{r}
summary(har.data)
head(har.data)
str(har.data)
```

No missing values. The distribution for the outcome variable looks acceptable. 

Problem idrntified: z4 is a factor but it has to be numeric as all other accelerometer readings. Fixing this now:

```{r}
har.data$z4 <- as.integer(har.data$z4)
str(har.data)
```

Now this is a nice dataset to work with. It has only numeric predictors and 5-level factor as outcome.

Let's check out the distribution of observations by class:
```{r}
table(har.data$class)
```
`sittingdown` and `standingup` have less number of observations than the other outcome values, but still a good number, not a rare event.


### Split the data

Since the dataset is long enough and not wide at all, the decision is to split it in 3 subsets - training, validation, and test. 

 - Training set - will be used to build 2 or 3 different models.
 - Validation set - will be used to compare the performance of several models and select one of them.
 - Testing set - to evaluate the selected model.
 
It is good practice to hold the test set, to treat is as data not available at this time, and use only for performance evaluation in the end.

Most of the time I will be using only the training set. The validation set will be used only to compare models, and the test set - only for final evaluation.

Creating the 3 sets here:

```{r}
set.seed(123)
idx <- createDataPartition(har.data$class, p = .8, list = FALSE)
tmp <- har.data[idx,]
har.test <- har.data[-idx,]
idx <- createDataPartition(tmp$class, p = .6, list = FALSE)
har.train <- tmp[idx,]
har.validation <- tmp[-idx,]

# check the number of observations for each set
c(nrow(har.train), nrow(har.validation), nrow(har.test))

# must be true
nrow(har.train) + nrow(har.validation) + nrow(har.test) == nrow(har.data)

```


### Brief exploration of the data

From now on, only the training data will be used for any analysis until the time comes to compare models. THen the validation set will be used for that. The test set will be only used for final evaluation of the model.


```{r}
summary(har.train)
```

Looking at the summary does not reveal anything extreme. 

Other things to check while exploring the training set are 
 - predictors with zero or near-zero variance
 - correlations between predictors

_Near-zero variance:_ 

Reviewing the data summary shows quite good variance, but still it is good to run a check. This is easy to do using `nearZeroVar()` from the `caret` package:

```{r}
nearZeroVar(har.train[,predictors], saveMetrics = TRUE)
```

The result shows no variables with zero or near-zero variance.

_Correlations:_

> This is related to the term `multicolinearity`; ref:
> http://stats.stackexchange.com/questions/86269/what-is-the-effect-of-having-correlated-predictors-in-a-multiple-regression-mode

```{r}
corMat <- cor(har.train[, predictors])
corrplot(corMat, method = "circle", type = "lower", order = "FPC", tl.cex = 0.75, tl.col = gray(.5))
```

This plot visually shows very strong correlation between x2, y2, and z2. Perhaps one of them will be sufficient, and the other two can be removed. Some models could benefit from avoiding highly correlated predictors.

There is also a `findCorrelations()` function in the `caret` package that helps find variables candidates for removal because of too strong correlation. How strong is too strong - this is defined by the function argument cutof:

```{r}
findCorrelation(corMat, cutoff = .95)
```


Based on the above results, x2 and z2 are candidates for removal. The following plots help visualize the relations between y2 and x2,z2 - the candidates for removal.

```{r}
par(mfrow = c(1, 2))
smoothScatter(har.train$y2, har.train$x2)
smoothScatter(har.train$y2, har.train$z2)
par(mfrow = c(1, 1))
```

Most points are on the diagonal line, which is visual hint for strong correlation.

For visual comparison, here are the plots using other variables, which are less correlated.

```{r}
par(mfrow = c(1, 2))
smoothScatter(har.train$x1, har.train$y1)
#smoothScatter(har.train$y3, har.train$x3)
smoothScatter(har.train$y3, har.train$x4)
par(mfrow = c(1, 1))
```

There are other methods to evaluate variable imortance as well. PCA can help with evaluating the variance explained; also some models can provide report about variable importance for the specific model.

Removing variables 4 and 6 (x2 and z2) because of the correlation finding:

```{r}
har.train <- har.train[,-6, -4]
predictors <- names(har.train[,1:11])
# check the result out
table(names(har.train[,1:11]), predictors)
```

Same variables removal should be done for the other two sets - either now or later before those sets are used:

```{r}
har.validation <- har.validation[,-6, -4]
har.test <- har.test[,-6, -4]
```

Check the correlation plot again:

```{r}
corMat <- cor(har.train[, predictors])
corrplot(corMat, method = "circle", type = "lower", order = "FPC", tl.cex = 0.75, tl.col = gray(.5))
findCorrelation(corMat, cutoff = .95)
```

Now x2 and y2 look highly correlated. Removing x2 based on that.

```{r}
har.train <- har.train[,-4]
predictors <- names(har.train[,1:10])
har.validation <- har.validation[,-4]
har.test <- har.test[,-4]
```

One more time, checking the correlations:

```{r}
corMat <- cor(har.train[, predictors])
corrplot(corMat, method = "circle", type = "lower", order = "FPC", tl.cex = 0.75, tl.col = gray(.5))
findCorrelation(corMat, cutoff = .95)
```

After that, the correlations look nice: there are no strongly correlated predictors. The dataset now has only 10 predictors plus the outcome `class`.



#### More visualizations

First get a subset for performance considerations.


```{r}
idx <- createDataPartition(har.train$class, p = .1, list = FALSE)
har.subset <- har.train[idx,]

```

### Modeling

In this section I am training and exploring a few different models, using the selected raw features.

```{r, message=FALSE}
model.saved = TRUE

fit.control <- trainControl(method='cv', 
                            number=7,
                            returnResamp='none', 
                            verboseIter = FALSE)
```

###### Random Forest Model

```{r, message=FALSE}
if( ! model.saved ) {
  set.seed(123)
  rf.model <- train(class ~ ., 
                    data = har.train,
                    method = "rf",
                    trControl = fit.control,
                    verbose = FALSE 
  )
  saveRDS(rf.model, "rf_model.rds")
} else {
  rf.model <- readRDS("rf_model.rds")
}

#rf.model$finalModel
plot(rf.model)
plot(varImp(rf.model, scale=F, tl.cex = 0.8))
```

#### GBM Model

```{r, message=FALSE}

if( ! model.saved ) {
#   gbm.grid <-  expand.grid(interaction.depth = c(4, 6, 8),
#                            n.trees = (15:25)*20,
#                            shrinkage = 0.1,
#                            n.minobsinnode = c(10,20))
  set.seed(123)
  gbm.model <- train(class ~ ., 
                     data = har.train,
                     method = "gbm",
                     trControl = fit.control,
                     verbose = FALSE)              # , tuneGrid = gbm.grid
  saveRDS(gbm.model, "gbm_model.rds")
} else {
  gbm.model <- readRDS("gbm_model.rds")
}

#gbm.model$bestTune
#gbm.model$finalModel
plot(gbm.model)
plot(varImp(gbm.model,scale=F, tl.cex = 0.8))
```

#### XGBTree Model

```{r, message=FALSE}

if( ! model.saved ) {
  set.seed(123)
  xgb.model <- train(class ~ ., 
                     data = har.train,
                     method = "xgbTree",
                     trControl = fit.control,
                     verbose = FALSE)              # , tuneGrid = gbm.grid
  saveRDS(xgb.model, "xgb_model.rds")
} else {
  xgb.model <- readRDS("xgb_model.rds")
}

#xgb.model$bestTune
#xgb.model$finalModel
plot(xgb.model)

#plot(varImp(xgb.model,scale=F, tl.cex = 0.8))

# par(mfrow = c(1, 3))
# plot(varImp(rf.model,scale=F, tl.cex = 0.8))
# plot(varImp(gbm.model,scale=F, tl.cex = 0.8))
# plot(varImp(xgb.model,scale=F, tl.cex = 0.8))
# par(mfrow = c(1, 1))


```

#### C5.0 Model

```{r, message=FALSE}

if( ! model.saved ) {
  set.seed(123)
  c50.model <- train(class ~ ., 
                     data = har.train,
                     method = "C5.0",
                     trControl = fit.control,
                     verbose = FALSE)              # , tuneGrid = gbm.grid
  saveRDS(c50.model, "c50_model.rds")
} else {
  c50.model <- readRDS("c50_model.rds")
}

#c50.model$bestTune
#c50.model$finalModel
plot(c50.model)
plot(varImp(c50.model,scale=F, tl.cex = 0.8))
```



### Evaluate and compare the models using the *validation* set

```{r, message=FALSE}
rf.val.pred <- predict(rf.model, newdata = har.validation)
rf.val.cm <- confusionMatrix(rf.val.pred, har.validation$class)

gbm.val.pred <- predict(gbm.model, newdata = har.validation)
gbm.val.cm <- confusionMatrix(gbm.val.pred, har.validation$class)

xgb.val.pred <- predict(xgb.model, newdata = har.validation)
xgb.val.cm <- confusionMatrix(xgb.val.pred, har.validation$class)

c50.val.pred <- predict(c50.model, newdata = har.validation)
c50.val.cm <- confusionMatrix(c50.val.pred, har.validation$class)
```


Evaluate and review the evaluation results for all the models, usint the validation set:

```{r, eval=FALSE}
rf.val.cm
gbm.val.cm 
xgb.val.cm
c50.val.cm
```
The result of the above chunk is skipped intentionally for readabilty considerations.

Now compare the accuracy and kappa values side by side.

```{r}
rf.accuracy <- rf.val.cm$overall[[1]]
rf.kappa <- rf.val.cm$overall[[2]]

gbm.accuracy <- gbm.val.cm$overall[[1]]
gbm.kappa <- gbm.val.cm$overall[[2]]

xgb.accuracy <- xgb.val.cm$overall[[1]]
xgb.kappa <- xgb.val.cm$overall[[2]]

c50.accuracy <- c50.val.cm$overall[[1]]
c50.kappa <- c50.val.cm$overall[[2]]

tb <- data.frame(c(rf.accuracy, gbm.accuracy, xgb.accuracy, c50.accuracy), 
                 c(rf.kappa, gbm.kappa, xgb.kappa, c50.kappa), 
                 row.names = c("RF", "GBM", "XGB", "C5.0"))
names(tb) <- c("accuracy", "cappa")

tb
```

### Evaluate the out of sample performance for the selected model

Using the `test` data set ...

```{r}
# use har.test instead of har.validation

c50.test.pred <- predict(c50.model, newdata = har.test)
c50.test.cm <- confusionMatrix(c50.test.pred, har.test$class)
c50.test.cm
```


### Conclusion

Starting from the original dataset, this study achieves performance measures very similar or practically the same to the one reported by the authors. 

Apart from the dataset, this study does not use any tools and techniques based on the original study. It applies  known techniques, common sense, and best practices.

With confirmed accuracy over 99%, the approach of attaching accelerometer-based sensors to the human body can be practically useful in variety of situations, for example automated remote healthcare monitoring of people in challenging health conditions.
